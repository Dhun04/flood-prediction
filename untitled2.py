# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qEL7BCGwMLZxY_8vJbDGaph3jRBUsVxM
"""

pip install overpy

import overpy
import geopandas as gpd
from shapely.geometry import LineString, Polygon

# Initialize the Overpass API
api = overpy.Overpass()

# Define the bounding box (replace with your desired coordinates)
bbox_south = 40.7128
bbox_west = -74.0060
bbox_north = 40.7739
bbox_east = -73.9339

# Define your Overpass API query
# The bbox is now formatted correctly with the coordinates
query = f"""
    [out:json];
    way["highway"]({bbox_south},{bbox_west},{bbox_north},{bbox_east});
    out body;
    >;
    out skel qt;
    way["waterway"]({bbox_south},{bbox_west},{bbox_north},{bbox_east});
    out body;
    >;
    out skel qt;
"""

# Execute the query
result = api.query(query)

# Convert results to GeoDataFrames
def ways_to_gdf(ways, type_):
    geometries = [LineString([(node.lon, node.lat) for node in way.nodes]) for way in ways]
    # Create a list of type values for each geometry
    types = [type_] * len(geometries)
    return gpd.GeoDataFrame(geometry=geometries, crs="EPSG:4326", data={"type": types})

roads_gdf = ways_to_gdf(result.ways, "road")
rivers_gdf = ways_to_gdf(result.ways, "river")

import geopandas as gpd

# Load road network shapefile
road_network = gpd.read_file('/content/surat_road_network.shp')

# Check the first few rows to understand the data
print(road_network.head())

pip install tensorflow keras numpy opencv-python matplotlib geopandas

pip install osmnx

pip install geopandas shapely rasterio

pip install osmnx geopandas

import osmnx as ox
import geopandas as gpd
import matplotlib.pyplot as plt

# Specify the location for Surat City
place_name = "Surat, Gujarat, India"

# Retrieve the road network for Surat City
# You can specify 'network_type' as 'drive', 'walk', 'bike', 'all', etc.
graph = ox.graph_from_place(place_name, network_type='all')

# Convert the graph to GeoDataFrames (for nodes and edges)
nodes, edges = ox.graph_to_gdfs(graph)

# Display the first few rows of the edges GeoDataFrame
print(edges.head())

# Check the columns in the edges GeoDataFrame
print(edges.columns)

# Check the coordinate reference system (CRS)
print(edges.crs)

# Plot the road network
edges.plot()
plt.title('Road Network of Surat City')
plt.show()

import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Define the directory where your images are stored
train_directory = 'path_to_flood_images/train'

# Initialize the ImageDataGenerator with preprocessing and augmentation
train_datagen = ImageDataGenerator(
    rescale=1.0/255,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)

# Create a data generator
train_generator = train_datagen.flow_from_directory(
    train_directory,
    target_size=(256, 256),  # Adjust size as needed
    batch_size=32,
    class_mode='binary'  # Use 'categorical' for multiple classes
)

import geopandas as gpd
from shapely.geometry import Point, box

# Load the road network data
road_network_fp = '/content/surat_road_network.shp'  # Replace with the path to your shapefile
road_network = gpd.read_file(road_network_fp)

# Define the latitude and longitude of the center point
lat, lon = 21.1702, 72.8311  # Example coordinates for Surat

# Create a Point object for the center
center_point = Point(lon, lat)

# Define the size of the bounding box (in degrees, adjust as needed)
bbox_size = 0.01  # Define the size of the bounding box

# Create a bounding box around the center point
bbox = box(center_point.x - bbox_size, center_point.y - bbox_size,
           center_point.x + bbox_size, center_point.y + bbox_size)

# Convert bounding box to GeoDataFrame
bbox_gdf = gpd.GeoDataFrame({'geometry': [bbox]}, crs='EPSG:4326')

# Filter the road network data to include only roads within the bounding box
roads_within_bbox = gpd.overlay(road_network, bbox_gdf, how='intersection')

# Save or display the result
roads_within_bbox.to_file('filtered_roads.shp')  # Save the filtered roads to a new shapefile
print(roads_within_bbox.head())  # Display the first few rows of the filtered data

import geopandas as gpd
from shapely.geometry import Point, box
import matplotlib.pyplot as plt

def filter_road_network_by_coordinates(shapefile_path, lat, lon, bbox_size=0.01):
    # Load the road network data
    road_network = gpd.read_file(shapefile_path)

    # Create a Point object for the center
    center_point = Point(lon, lat)

    # Create a bounding box around the center point
    bbox = box(center_point.x - bbox_size, center_point.y - bbox_size,
               center_point.x + bbox_size, center_point.y + bbox_size)

    # Convert bounding box to GeoDataFrame
    bbox_gdf = gpd.GeoDataFrame({'geometry': [bbox]}, crs='EPSG:4326')

    # Filter the road network data to include only roads within the bounding box
    roads_within_bbox = gpd.overlay(road_network, bbox_gdf, how='intersection')

    # Save or display the result
    output_fp = 'filtered_roads.shp'
    roads_within_bbox.to_file(output_fp)  # Save the filtered roads to a new shapefile

    # Plot the filtered road network
    fig, ax = plt.subplots(figsize=(10, 10))
    roads_within_bbox.plot(ax=ax, color='blue', linewidth=0.5)
    plt.title('Filtered Road Network Around Specified Coordinates')
    plt.xlabel('Longitude')
    plt.ylabel('Latitude')
    plt.show()

    # Return the path to the saved shapefile
    return output_fp

# Example usage
if __name__ == "__main__":
    shapefile_path = '/content/surat_road_network.shp'  # Replace with the path to your shapefile

    # Get user input
    lat = float(input("Enter the latitude of the center point: "))
    lon = float(input("Enter the longitude of the center point: "))
    bbox_size = float(input("Enter the size of the bounding box (in degrees): "))

    # Call the function with user input
    filtered_shapefile = filter_road_network_by_coordinates(shapefile_path, lat, lon, bbox_size)
    print(f"Filtered road network saved to: {filtered_shapefile}")

pip install numpy pandas tensorflow scikit-learn matplotlib

import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator

# Load the data
data = pd.read_csv('/content/Surat,India.csv')

# Convert 'datetime' to datetime format and set as index
data['datetime'] = pd.to_datetime(data['datetime'])
data.set_index('datetime', inplace=True)

# Handle missing values
imputer = SimpleImputer(strategy='mean')
data[['tempmax', 'tempmin', 'temp', 'feelslikemax', 'feelslikemin', 'feelslike',
      'dew', 'humidity', 'precip', 'precipprob', 'precipcover', 'snow', 'snowdepth',
      'windgust', 'windspeed', 'sealevelpressure', 'cloudcover', 'visibility',
      'solarradiation', 'solarenergy', 'uvindex']] = imputer.fit_transform(
    data[['tempmax', 'tempmin', 'temp', 'feelslikemax', 'feelslikemin', 'feelslike',
          'dew', 'humidity', 'precip', 'precipprob', 'precipcover', 'snow', 'snowdepth',
          'windgust', 'windspeed', 'sealevelpressure', 'cloudcover', 'visibility',
          'solarradiation', 'solarenergy', 'uvindex']]
)

# Encode categorical features (one-hot encoding)
data = pd.get_dummies(data, columns=['preciptype'])

# Normalize features
scaler = MinMaxScaler()
numerical_features = ['tempmax', 'tempmin', 'temp', 'feelslikemax', 'feelslikemin',
                       'feelslike', 'dew', 'humidity', 'precip', 'precipprob', 'precipcover',
                       'snow', 'snowdepth', 'windgust', 'windspeed', 'sealevelpressure',
                       'cloudcover', 'visibility', 'solarradiation', 'solarenergy', 'uvindex']
data[numerical_features] = scaler.fit_transform(data[numerical_features])

# Define features and target for LSTM
sequence_length = 7  # Number of days to use for training
num_features = len(numerical_features) + len(data.filter(like='preciptype').columns)

# Prepare sequences
def create_sequences(data, seq_length):
    xs, ys = [], []
    for i in range(len(data) - seq_length):
        x = data.iloc[i:i + seq_length].values
        y = data.iloc[i + seq_length]['temp']  # Example target variable (temperature)
        xs.append(x)
        ys.append(y)
    return np.array(xs), np.array(ys)

X, y = create_sequences(data, sequence_length)

# Split data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout

# Build the LSTM model
model = Sequential([
    LSTM(50, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True),
    LSTM(50, activation='relu', return_sequences=False),
    Dropout(0.2),
    Dense(1)  # Output layer for temperature prediction
])

# Compile the model
model.compile(optimizer='adam', loss='mse')

# Train the model
history = model.fit(
    X_train, y_train,
    epochs=50,
    batch_size=32,
    validation_split=0.1,
    verbose=1
)

# Evaluate the model
loss = model.evaluate(X_test, y_test)
print(f'Test Loss: {loss}')

# Check the types of your arrays
print(type(X_train))
print(type(y_train))

# Check the shapes of your arrays
print(X_train.shape)
print(y_train.shape)

# Ensure they are numpy arrays
X_train = np.array(X_train)
y_train = np.array(y_train)

# Ensure that X_train and y_train have the correct dimensions
print(X_train.shape)  # Should be (num_samples, sequence_length, num_features)
print(y_train.shape)  # Should be (num_samples,)

import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer

# Load the data
data = pd.read_csv('/content/Surat,India.csv')

# Drop the non-numerical "Location name" column
data.drop(columns=['name'], inplace=True)

# Convert 'datetime' to datetime format
data['datetime'] = pd.to_datetime(data['datetime'])

# Extract useful features from datetime
data['hour'] = data['datetime'].dt.hour
data['day_of_week'] = data['datetime'].dt.dayofweek
data['month'] = data['datetime'].dt.month

# Drop the original datetime column to avoid the conversion error
data.drop(columns=['datetime'], inplace=True)

# Handle missing values
imputer = SimpleImputer(strategy='mean')
columns_to_impute = ['tempmax', 'tempmin', 'temp', 'feelslikemax', 'feelslikemin', 'feelslike',
                     'dew', 'humidity', 'precip', 'precipprob', 'precipcover', 'snow', 'snowdepth',
                     'windgust', 'windspeed', 'sealevelpressure', 'cloudcover', 'visibility',
                     'solarradiation', 'solarenergy', 'uvindex']

data[columns_to_impute] = imputer.fit_transform(data[columns_to_impute])

# Encode categorical features (one-hot encoding)
data = pd.get_dummies(data, columns=['preciptype'])

# Normalize features
scaler = MinMaxScaler()
numerical_features = columns_to_impute + ['hour', 'day_of_week', 'month']

data[numerical_features] = scaler.fit_transform(data[numerical_features])

# Define features and target for LSTM
sequence_length = 7  # Number of days to use for training
num_features = len(numerical_features) + len(data.filter(like='preciptype').columns)

# Prepare sequences
def create_sequences(data, seq_length):
    xs, ys = [], []
    for i in range(len(data) - seq_length):
        x = data.iloc[i:i + seq_length].values
        y = data.iloc[i + seq_length]['temp']  # Example target variable (temperature)
        xs.append(x)
        ys.append(y)
    return np.array(xs), np.array(ys)

X, y = create_sequences(data, sequence_length)

# Convert to float32 instead of float16 for TensorFlow compatibility
X = X.astype(np.float32)
y = y.astype(np.float32)

# Split data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)

# Inspect the first few rows of the dataset
print(data.head())

# Check the data types of each column
print(data.dtypes)

import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer

# Load the data
data = pd.read_csv('/content/Surat,India.csv')

# Drop the non-numerical columns that are not needed
data.drop(columns=['name', 'sunrise', 'sunset', 'conditions', 'description', 'icon', 'stations'], inplace=True)

# Convert 'datetime' to datetime format
data['datetime'] = pd.to_datetime(data['datetime'])

# Extract useful features from datetime
data['hour'] = data['datetime'].dt.hour
data['day_of_week'] = data['datetime'].dt.dayofweek
data['month'] = data['datetime'].dt.month

# Drop the original datetime column
data.drop(columns=['datetime'], inplace=True)

# Check if 'preciptype' exists, if so, apply one-hot encoding
if 'preciptype' in data.columns:
    data = pd.get_dummies(data, columns=['preciptype'], drop_first=True)

# Handle missing values
imputer = SimpleImputer(strategy='mean')
columns_to_impute = ['tempmax', 'tempmin', 'temp', 'feelslikemax', 'feelslikemin', 'feelslike',
                     'dew', 'humidity', 'precip', 'precipprob', 'precipcover', 'snow', 'snowdepth',
                     'windgust', 'windspeed', 'winddir', 'sealevelpressure', 'cloudcover', 'visibility',
                     'solarradiation', 'solarenergy', 'uvindex', 'severerisk', 'moonphase']

data[columns_to_impute] = imputer.fit_transform(data[columns_to_impute])

# Normalize features
scaler = MinMaxScaler()
numerical_features = columns_to_impute + ['hour', 'day_of_week', 'month']

# Ensure only numeric columns are selected for scaling
data[numerical_features] = scaler.fit_transform(data[numerical_features])

# Ensure all columns are numeric
print("Column types after encoding and scaling:")
print(data.dtypes)

# Define features and target for LSTM
sequence_length = 7  # Number of days to use for training

# Prepare sequences
def create_sequences(data, seq_length):
    xs, ys = [], []
    for i in range(len(data) - seq_length):
        x = data.iloc[i:i + seq_length].values
        y = data.iloc[i + seq_length]['temp']  # Example target variable (temperature)
        xs.append(x)
        ys.append(y)
    return np.array(xs), np.array(ys)

X, y = create_sequences(data, sequence_length)

# Convert to float32 instead of float16 for TensorFlow compatibility
X = X.astype(np.float32)
y = y.astype(np.float32)

# Split data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout

# Define the LSTM model
model = Sequential()

# Add LSTM layer
model.add(LSTM(units=50, return_sequences=True, input_shape=(sequence_length, X_train.shape[2])))
model.add(Dropout(0.2))

# Add another LSTM layer
model.add(LSTM(units=50, return_sequences=False))
model.add(Dropout(0.2))

# Add a Dense layer
model.add(Dense(units=25))
model.add(Dropout(0.2))

# Output layer (single neuron for regression output)
model.add(Dense(units=1))

# Compile the model
model.compile(optimizer='adam', loss='mean_squared_error')

# Display the model architecture
model.summary()

# Train the model
history = model.fit(
    X_train, y_train,
    epochs=50,
    batch_size=32,
    validation_data=(X_test, y_test)
)

# Evaluate the model on the test set
test_loss = model.evaluate(X_test, y_test)
print(f"Test Loss: {test_loss}")

# Predict the next 3 days
last_sequence = X_test[-1]  # Take the last sequence from the test data
predicted_temps = []

for _ in range(3):
    # Predict the temperature
    pred_temp = model.predict(last_sequence.reshape(1, sequence_length, X_train.shape[2]))
    predicted_temps.append(pred_temp[0, 0])

    # Update the last sequence with the predicted temperature
    last_sequence = np.roll(last_sequence, -1, axis=0)  # Shift the sequence
    last_sequence[-1, 0] = pred_temp  # Replace the last value with the predicted temp

print("Predicted temperatures for the next 3 days:", predicted_temps)

import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout

# Load the data
data = pd.read_csv('/content/Surat,India.csv')

# Drop non-numerical columns that are not needed
data.drop(columns=['name', 'sunrise', 'sunset', 'description', 'icon', 'stations'], inplace=True)

# Convert 'datetime' to datetime format
data['datetime'] = pd.to_datetime(data['datetime'])

# Extract useful features from datetime
data['hour'] = data['datetime'].dt.hour
data['day_of_week'] = data['datetime'].dt.dayofweek
data['month'] = data['datetime'].dt.month

# Drop the original datetime column
data.drop(columns=['datetime'], inplace=True)

# Handle missing values
imputer = SimpleImputer(strategy='mean')
columns_to_impute = ['tempmax', 'tempmin', 'temp', 'feelslikemax', 'feelslikemin', 'feelslike',
                     'dew', 'humidity', 'precip', 'precipprob', 'precipcover', 'snow', 'snowdepth',
                     'windgust', 'windspeed', 'winddir', 'sealevelpressure', 'cloudcover', 'visibility',
                     'solarradiation', 'solarenergy', 'uvindex', 'severerisk', 'moonphase']

data[columns_to_impute] = imputer.fit_transform(data[columns_to_impute])

# Encode categorical features (e.g., 'preciptype', 'conditions')
data = pd.get_dummies(data, columns=['preciptype', 'conditions'], drop_first=True)

# Normalize features
scaler = MinMaxScaler()
numerical_features = columns_to_impute + ['hour', 'day_of_week', 'month']

# Ensure only numeric columns are selected for scaling
data[numerical_features] = scaler.fit_transform(data[numerical_features])

# Define features and targets for LSTM
sequence_length = 7  # Number of days to use for training

# Prepare sequences
def create_sequences(data, seq_length, target_columns):
    xs, ys = [], []
    for i in range(len(data) - seq_length):
        x = data.iloc[i:i + seq_length].values
        y = data.iloc[i + seq_length][target_columns].values  # Target columns for multi-output
        xs.append(x)
        ys.append(y)
    return np.array(xs), np.array(ys)

# Target columns for prediction
target_columns = ['precip', 'precipprob', 'precipcover', 'windspeed'] + list(data.filter(like='preciptype').columns) + list(data.filter(like='conditions').columns)

X, y = create_sequences(data, sequence_length, target_columns)

# Convert to float32
X = X.astype(np.float32)
y = y.astype(np.float32)

# Split data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)

# Define the LSTM model for multi-output
model = Sequential()

# Add LSTM layer
model.add(LSTM(units=50, return_sequences=True, input_shape=(sequence_length, X_train.shape[2])))
model.add(Dropout(0.2))

# Add another LSTM layer
model.add(LSTM(units=50, return_sequences=False))
model.add(Dropout(0.2))

# Add a Dense layer
model.add(Dense(units=25))
model.add(Dropout(0.2))

# Output layer with neurons for each target variable
model.add(Dense(units=len(target_columns)))

# Compile the model
model.compile(optimizer='adam', loss='mean_squared_error')

# Train the model
history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))

# Evaluate the model on the test set
test_loss = model.evaluate(X_test, y_test)
print(f"Test Loss: {test_loss}")

# Predict the next 3 days of precipitation, wind speed, etc.
last_sequence = X_test[-1]  # Take the last sequence from the test data
predicted_weather = []

for _ in range(3):
    # Predict precipitation, wind speed, etc.
    pred_weather = model.predict(last_sequence.reshape(1, sequence_length, X_train.shape[2]))
    predicted_weather.append(pred_weather[0])

    # Update the last sequence with the predicted values
    last_sequence = np.roll(last_sequence, -1, axis=0)  # Shift the sequence
    last_sequence[-1, :len(target_columns)] = pred_weather  # Replace the last values with the predicted values

# Print the predicted weather for the next 3 days
predicted_weather = np.array(predicted_weather)
print("Predicted weather for the next 3 days:")
print(predicted_weather)

import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler, OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout

# Load the data
data = pd.read_csv('/content/Surat,India.csv')

# Drop non-numerical columns that are not needed
data.drop(columns=['name', 'sunrise', 'sunset', 'description', 'icon', 'stations'], inplace=True)

# Convert 'datetime' to datetime format
data['datetime'] = pd.to_datetime(data['datetime'])

# Extract useful features from datetime
data['hour'] = data['datetime'].dt.hour
data['day_of_week'] = data['datetime'].dt.dayofweek
data['month'] = data['datetime'].dt.month

# Drop the original datetime column
data.drop(columns=['datetime'], inplace=True)

# Handle missing values
imputer = SimpleImputer(strategy='mean')
columns_to_impute = ['tempmax', 'tempmin', 'temp', 'feelslikemax', 'feelslikemin', 'feelslike',
                     'dew', 'humidity', 'precip', 'precipprob', 'precipcover', 'snow', 'snowdepth',
                     'windgust', 'windspeed', 'winddir', 'sealevelpressure', 'cloudcover', 'visibility',
                     'solarradiation', 'solarenergy', 'uvindex', 'severerisk', 'moonphase']

data[columns_to_impute] = imputer.fit_transform(data[columns_to_impute])

# Encode categorical features (e.g., 'preciptype', 'conditions')
encoder_preciptype = OneHotEncoder(sparse=False, drop='first')
encoded_preciptype = encoder_preciptype.fit_transform(data[['preciptype']])

encoder_conditions = OneHotEncoder(sparse=False, drop='first')
encoded_conditions = encoder_conditions.fit_transform(data[['conditions']])

# Create new DataFrame for encoded features
encoded_preciptype_df = pd.DataFrame(encoded_preciptype, columns=encoder_preciptype.get_feature_names_out(['preciptype']))
encoded_conditions_df = pd.DataFrame(encoded_conditions, columns=encoder_conditions.get_feature_names_out(['conditions']))

# Concatenate the encoded features with the original data
data = pd.concat([data, encoded_preciptype_df, encoded_conditions_df], axis=1)
data.drop(columns=['preciptype', 'conditions'], inplace=True)

# Normalize features
scaler = MinMaxScaler()
numerical_features = columns_to_impute + ['hour', 'day_of_week', 'month'] + list(encoded_preciptype_df.columns) + list(encoded_conditions_df.columns)
data[numerical_features] = scaler.fit_transform(data[numerical_features])

# Define features and targets for LSTM
sequence_length = 7  # Number of days to use for training

# Prepare sequences
def create_sequences(data, seq_length, target_columns):
    xs, ys = [], []
    for i in range(len(data) - seq_length):
        x = data.iloc[i:i + seq_length].values
        y = data.iloc[i + seq_length][target_columns].values  # Target columns for multi-output
        xs.append(x)
        ys.append(y)
    return np.array(xs), np.array(ys)

# Target columns for prediction
target_columns = ['precip', 'precipprob', 'precipcover', 'windspeed'] + list(encoded_preciptype_df.columns) + list(encoded_conditions_df.columns)

X, y = create_sequences(data, sequence_length, target_columns)

# Convert to float32
X = X.astype(np.float32)
y = y.astype(np.float32)

# Split data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)

# Define the LSTM model for multi-output
model = Sequential()
model.add(LSTM(units=50, return_sequences=True, input_shape=(sequence_length, X_train.shape[2])))
model.add(Dropout(0.2))
model.add(LSTM(units=50, return_sequences=False))
model.add(Dropout(0.2))
model.add(Dense(units=25))
model.add(Dropout(0.2))
model.add(Dense(units=len(target_columns)))

# Compile the model
model.compile(optimizer='adam', loss='mean_squared_error')

# Train the model
history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))

# Evaluate the model on the test set
test_loss = model.evaluate(X_test, y_test)
print(f"Test Loss: {test_loss}")

# Predict the next 3 days of precipitation, wind speed, etc.
last_sequence = X_test[-1]  # Take the last sequence from the test data
predicted_weather = []

for _ in range(3):
    pred_weather = model.predict(last_sequence.reshape(1, sequence_length, X_train.shape[2]))
    predicted_weather.append(pred_weather[0])
    last_sequence = np.roll(last_sequence, -1, axis=0)
    last_sequence[-1, :len(target_columns)] = pred_weather

# Convert predictions to DataFrame
predicted_weather = np.array(predicted_weather)
predicted_weather_df = pd.DataFrame(predicted_weather, columns=target_columns)

# Decode the one-hot encoded columns
def decode_one_hot(encoded_df, encoder):
    decoded_list = []
    for index, row in encoded_df.iterrows():
        decoded = [encoder.categories_[0][i] for i, val in enumerate(row) if val == 1]
        decoded_list.append(decoded[0] if decoded else 'Unknown')
    return decoded_list

# Decode one-hot encoded predictions
predicted_weather_df['preciptype'] = decode_one_hot(predicted_weather_df[list(encoded_preciptype_df.columns)], encoder_preciptype)
predicted_weather_df['conditions'] = decode_one_hot(predicted_weather_df[list(encoded_conditions_df.columns)], encoder_conditions)

# Print the predicted weather for the next 3 days
print("Predicted weather for the next 3 days:")
print(predicted_weather_df[['precip', 'precipprob', 'precipcover', 'windspeed', 'preciptype', 'conditions']])

predicted_weather_df.plot(kind='bar', figsize=(12, 6))
plt.title('Predicted Weather for Next 3 Days')
plt.xlabel('Days')
plt.ylabel('Values')
plt.show()

import requests
from datetime import datetime

# Function to fetch weather data
def fetch_weather(api_key, city):
    url = f"http://api.openweathermap.org/data/2.5/weather?q={city}&appid={api_key}&units=metric"
    response = requests.get(url)
    data = response.json()
    return data

# Function to display weather information
def display_weather(data):
    # Extract weather information
    temp = data['main']['temp']
    humidity = data['main']['humidity']
    sunrise = datetime.fromtimestamp(data['sys']['sunrise']).strftime('%Y-%m-%d %H:%M:%S')
    sunset = datetime.fromtimestamp(data['sys']['sunset']).strftime('%Y-%m-%d %H:%M:%S')

    # Display weather information
    print(f"Current Temperature: {temp}°C")
    print(f"Humidity: {humidity}%")
    print(f"Sunrise Time: {sunrise}")
    print(f"Sunset Time: {sunset}")

# Main script
if __name__ == "__main__":
    api_key = "fc81644584d6d3332ec0a4370dea20a8"  # Replace with your OpenWeatherMap API key
    city = "Surat"  # Replace with the desired city

    # Fetch and display weather information
    weather_data = fetch_weather(api_key, city)
    display_weather(weather_data)

!pip install ipywidgets

import pandas as pd
import numpy as np
import ipywidgets as widgets
from IPython.display import display, clear_output
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from sklearn.preprocessing import MinMaxScaler, OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
import matplotlib.pyplot as plt

def display_predictions(predictions_df):
    # Create widgets for displaying results
    output = widgets.Output()
    with output:
        clear_output(wait=True)
        print("Predicted Weather for the Next 3 Days:")
        display(predictions_df)

    display(output)

# Make predictions and create DataFrame
last_sequence = X_test[-1]
predicted_weather = []

for _ in range(3):
    pred_weather = model.predict(last_sequence.reshape(1, sequence_length, X_train.shape[2]))
    predicted_weather.append(pred_weather[0])
    last_sequence = np.roll(last_sequence, -1, axis=0)
    last_sequence[-1, :len(target_columns)] = pred_weather

predicted_weather = np.array(predicted_weather)
predicted_weather_df = pd.DataFrame(predicted_weather, columns=target_columns)
predicted_weather_df['preciptype'] = decode_one_hot(predicted_weather_df[list(encoded_preciptype_df.columns)], encoder_preciptype)
predicted_weather_df['conditions'] = decode_one_hot(predicted_weather_df[list(encoded_conditions_df.columns)], encoder_conditions)
predicted_weather_df = predicted_weather_df[['precip', 'precipprob', 'precipcover', 'windspeed', 'preciptype', 'conditions']]

# Display predictions
display_predictions(predicted_weather_df)

def decode_one_hot(encoded_df, encoder):
    return encoder.inverse_transform(encoded_df)

# Initialize OneHotEncoders for 'preciptype' and 'conditions'
encoder_preciptype = OneHotEncoder(sparse=False)
encoder_conditions = OneHotEncoder(sparse=False)

# Fit the encoders on the data
encoder_preciptype.fit(data.filter(like='preciptype'))
encoder_conditions.fit(data.filter(like='conditions'))

def display_predictions(predictions_df):
    output = widgets.Output()
    with output:
        clear_output(wait=True)
        print("Predicted Weather for the Next 3 Days:")
        display(predictions_df)

    display(output)

!pip install pandas scikit-learn

import pandas as pd
import numpy as np
from sklearn.preprocessing import OneHotEncoder
from tensorflow.keras.models import Sequential
# ... (import other necessary modules)

# Assuming 'data' is your original DataFrame, 'model' is your trained model,
# and 'sequence_length' is defined as per your model architecture

def display_predictions(predictions_df):
    # Create widgets for displaying results (if using Jupyter Notebook)
    output = widgets.Output()
    with output:
        clear_output(wait=True)
        print("Predicted Weather for the Next 3 Days:")
        display(predictions_df)

    display(output)

def decode_one_hot(encoded_df, encoder):
    return encoder.inverse_transform(encoded_df)

# Initialize OneHotEncoders for 'preciptype' and 'conditions'
encoder_preciptype = OneHotEncoder(sparse=False)
encoder_conditions = OneHotEncoder(sparse=False)

# Fit the encoders on the original data
encoder_preciptype.fit(data.filter(like='preciptype'))
encoder_conditions.fit(data.filter(like='conditions'))

# Make predictions and create DataFrame
last_sequence = X_test[-1]
predicted_weather = []

for _ in range(3):
    pred_weather = model.predict(last_sequence.reshape(1, sequence_length, X_train.shape[2]))
    predicted_weather.append(pred_weather[0])
    last_sequence = np.roll(last_sequence, -1, axis=0)
    last_sequence[-1, :len(target_columns)] = pred_weather

predicted_weather = np.array(predicted_weather)
predicted_weather_df = pd.DataFrame(predicted_weather, columns=target_columns)

# Get one-hot encoded representations before decoding
encoded_preciptype_df = pd.DataFrame(encoder_preciptype.transform(predicted_weather_df[['preciptype']]),
                                    columns=encoder_preciptype.get_feature_names_out(['preciptype']))
encoded_conditions_df = pd.DataFrame(encoder_conditions.transform(predicted_weather_df[['conditions']]),
                                     columns=encoder_conditions.get_feature_names_out(['conditions']))

# Decode one-hot encoded columns
predicted_weather_df['preciptype'] = decode_one_hot(encoded_preciptype_df, encoder_preciptype)
predicted_weather_df['conditions'] = decode_one_hot(encoded_conditions_df, encoder_conditions)

# Reorder columns for better readability
predicted_weather_df = predicted_weather_df[['precip', 'precipprob', 'precipcover', 'windspeed', 'preciptype', 'conditions']]

# Display predictions
display_predictions(predicted_weather_df)

# Display predictions using widgets
display_predictions(predicted_weather_df)

# Load the data
data = pd.read_csv('/content/Surat,India.csv')

# Drop non-numerical columns that are not needed
data.drop(columns=['name', 'sunrise', 'sunset', 'description', 'icon', 'stations'], inplace=True)

# Convert 'datetime' to datetime format
data['datetime'] = pd.to_datetime(data['datetime'])

# Extract useful features from datetime
data['hour'] = data['datetime'].dt.hour
data['day_of_week'] = data['datetime'].dt.dayofweek
data['month'] = data['datetime'].dt.month

# Drop the original datetime column
data.drop(columns=['datetime'], inplace=True)

# Handle missing values
imputer = SimpleImputer(strategy='mean')
columns_to_impute = ['tempmax', 'tempmin', 'temp', 'feelslikemax', 'feelslikemin', 'feelslike',
                     'dew', 'humidity', 'precip', 'precipprob', 'precipcover', 'snow', 'snowdepth',
                     'windgust', 'windspeed', 'winddir', 'sealevelpressure', 'cloudcover', 'visibility',
                     'solarradiation', 'solarenergy', 'uvindex', 'severerisk', 'moonphase']

data[columns_to_impute] = imputer.fit_transform(data[columns_to_impute])

# Encode categorical features
data = pd.get_dummies(data, columns=['preciptype', 'conditions'], drop_first=True)

# Normalize features
scaler = MinMaxScaler()
numerical_features = columns_to_impute + ['hour', 'day_of_week', 'month']
data[numerical_features] = scaler.fit_transform(data[numerical_features])

def create_sequences(data, seq_length, target_columns):
    xs, ys = [], []
    for i in range(len(data) - seq_length):
        x = data.iloc[i:i + seq_length].values
        y = data.iloc[i + seq_length][target_columns].values  # Target columns for multi-output
        xs.append(x)
        ys.append(y)
    return np.array(xs), np.array(ys)

# Define target columns
target_columns = ['precip', 'precipprob', 'precipcover', 'windspeed'] + list(data.filter(like='preciptype').columns) + list(data.filter(like='conditions').columns)

# Prepare sequences
sequence_length = 7
X, y = create_sequences(data, sequence_length, target_columns)

# Convert to float32
X = X.astype(np.float32)
y = y.astype(np.float32)

# Split data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)

model = Sequential()
model.add(LSTM(units=50, return_sequences=True, input_shape=(sequence_length, X_train.shape[2])))
model.add(Dropout(0.2))
model.add(LSTM(units=50, return_sequences=False))
model.add(Dropout(0.2))
model.add(Dense(units=25))
model.add(Dropout(0.2))
model.add(Dense(units=len(target_columns)))
model.compile(optimizer='adam', loss='mean_squared_error')

# Train the model
history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))

def decode_one_hot(encoded_df, encoder):
    if len(encoded_df) == 0:
        return np.array([])
    return encoder.inverse_transform(encoded_df)

# Initialize OneHotEncoders
encoder_preciptype = OneHotEncoder(sparse=False)
encoder_conditions = OneHotEncoder(sparse=False)

# Fit encoders on the data
encoder_preciptype.fit(data.filter(like='preciptype'))
encoder_conditions.fit(data.filter(like='conditions'))

import pandas as pd
import numpy as np
import ipywidgets as widgets
from IPython.display import display, clear_output
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from sklearn.preprocessing import MinMaxScaler, OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer

# Load and preprocess the data (assuming data is already preprocessed)
# This example assumes that the data has been preprocessed and split into training and test sets.

# Function to decode one-hot encoded features
def decode_one_hot(encoded_data, encoder):
    if encoded_data.size == 0:
        return np.array([])
    return encoder.inverse_transform(encoded_data)

# Initialize OneHotEncoders
encoder_preciptype = OneHotEncoder(sparse=False, handle_unknown='ignore')
encoder_conditions = OneHotEncoder(sparse=False, handle_unknown='ignore')

# Fit encoders on the data (assuming you have data for fitting)
encoder_preciptype.fit(data.filter(like='preciptype'))
encoder_conditions.fit(data.filter(like='conditions'))

# Define and train the LSTM model
sequence_length = 7
model = Sequential()
model.add(LSTM(units=50, return_sequences=True, input_shape=(sequence_length, X_train.shape[2])))
model.add(Dropout(0.2))
model.add(LSTM(units=50, return_sequences=False))
model.add(Dropout(0.2))
model.add(Dense(units=25))
model.add(Dropout(0.2))
model.add(Dense(units=len(target_columns)))
model.compile(optimizer='adam', loss='mean_squared_error')

# Train the model
history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))

# Function to display predictions
def display_predictions(predictions_df):
    output = widgets.Output()
    with output:
        clear_output(wait=True)
        print("Predicted Weather for the Next 3 Days:")
        display(predictions_df)
    display(output)

# Make predictions
last_sequence = X_test[-1]
predicted_weather = []

for _ in range(3):
    pred_weather = model.predict(last_sequence.reshape(1, sequence_length, X_train.shape[2]))
    predicted_weather.append(pred_weather[0])
    last_sequence = np.roll(last_sequence, -1, axis=0)
    last_sequence[-1, :len(target_columns)] = pred_weather

predicted_weather = np.array(predicted_weather)
predicted_weather_df = pd.DataFrame(predicted_weather, columns=target_columns)

# Decode one-hot encoded columns
if 'preciptype' in predicted_weather_df.columns:
    preciptype_cols = list(data.filter(like='preciptype').columns)
    predicted_weather_df[preciptype_cols] = decode_one_hot(predicted_weather_df[preciptype_cols].values, encoder_preciptype)

if 'conditions' in predicted_weather_df.columns:
    conditions_cols = list(data.filter(like='conditions').columns)
    predicted_weather_df[conditions_cols] = decode_one_hot(predicted_weather_df[conditions_cols].values, encoder_conditions)

# Reorder columns for better readability
ordered_columns = ['precip', 'precipprob', 'precipcover', 'windspeed'] + preciptype_cols + conditions_cols
predicted_weather_df = predicted_weather_df[ordered_columns]

# Display predictions
display_predictions(predicted_weather_df)

import pandas as pd
import numpy as np
import ipywidgets as widgets
from IPython.display import display, clear_output
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from sklearn.preprocessing import MinMaxScaler, OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer

# Load and preprocess the data (assuming data is already preprocessed)

# Define the function to decode one-hot encoded features
def decode_one_hot(encoded_data, encoder):
    if encoded_data.size == 0:
        return np.array([])
    return encoder.inverse_transform(encoded_data)

# Initialize OneHotEncoders
encoder_preciptype = OneHotEncoder(sparse=False, handle_unknown='ignore')
encoder_conditions = OneHotEncoder(sparse=False, handle_unknown='ignore')

# Fit encoders on the data (assuming you have data for fitting)
encoder_preciptype.fit(data.filter(like='preciptype'))
encoder_conditions.fit(data.filter(like='conditions'))

# Define and train the LSTM model
sequence_length = 7
model = Sequential()
model.add(LSTM(units=50, return_sequences=True, input_shape=(sequence_length, X_train.shape[2])))
model.add(Dropout(0.2))
model.add(LSTM(units=50, return_sequences=False))
model.add(Dropout(0.2))
model.add(Dense(units=25))
model.add(Dropout(0.2))
model.add(Dense(units=len(target_columns)))
model.compile(optimizer='adam', loss='mean_squared_error')

# Train the model
history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))

# Function to display predictions with a UI
def display_predictions_ui(predictions_df):
    output = widgets.Output()
    with output:
        clear_output(wait=True)
        print("Predicted Weather for the Next 3 Days:")
        display(predictions_df)
    display(output)

# Make predictions
last_sequence = X_test[-1]
predicted_weather = []

for _ in range(3):
    pred_weather = model.predict(last_sequence.reshape(1, sequence_length, X_train.shape[2]))
    predicted_weather.append(pred_weather[0])
    last_sequence = np.roll(last_sequence, -1, axis=0)
    last_sequence[-1, :len(target_columns)] = pred_weather

predicted_weather = np.array(predicted_weather)
predicted_weather_df = pd.DataFrame(predicted_weather, columns=target_columns)

# Decode one-hot encoded columns
if 'preciptype' in predicted_weather_df.columns:
    preciptype_cols = list(data.filter(like='preciptype').columns)
    predicted_weather_df[preciptype_cols] = decode_one_hot(predicted_weather_df[preciptype_cols].values, encoder_preciptype)

if 'conditions' in predicted_weather_df.columns:
    conditions_cols = list(data.filter(like='conditions').columns)
    predicted_weather_df[conditions_cols] = decode_one_hot(predicted_weather_df[conditions_cols].values, encoder_conditions)

# Reorder columns for better readability
ordered_columns = ['precip', 'precipprob', 'precipcover', 'windspeed'] + preciptype_cols + conditions_cols
predicted_weather_df = predicted_weather_df[ordered_columns]

# Display predictions
display_predictions_ui(predicted_weather_df)

print("Shape of y_test:", y_test.shape)
print("Shape of predicted_weather:", predicted_weather.shape)

# Define and train the LSTM model with 5 output features
sequence_length = 7
model = Sequential()
model.add(LSTM(units=50, return_sequences=True, input_shape=(sequence_length, X_train.shape[2])))
model.add(Dropout(0.2))
model.add(LSTM(units=50, return_sequences=False))
model.add(Dropout(0.2))
model.add(Dense(units=25))
model.add(Dropout(0.2))
model.add(Dense(units=5))  # Update to 5 features
model.compile(optimizer='adam', loss='mean_squared_error')

# Train the model
history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))

# Ensure predictions match the shape of y_test
# Example code to adjust dimensions
predicted_weather_df = pd.DataFrame(predicted_weather, columns=['precip', 'precipprob', 'precipcover', 'windspeed'])

# Add placeholders for missing features in predictions (e.g., 'snow', 'other_feature')
missing_features = ['snow', 'other_feature']  # Replace with actual feature names if needed
for feature in missing_features:
    predicted_weather_df[feature] = 0  # Assuming default value as 0

# Ensure correct ordering of columns in predictions_df to match y_test
predicted_weather_df = predicted_weather_df[['precip', 'precipprob', 'precipcover', 'windspeed'] + missing_features]

# Flatten y_test and predictions
y_test_flat = y_test.reshape(-1, len(predicted_weather_df.columns))
predictions_flat = predicted_weather_df.values.reshape(-1, len(predicted_weather_df.columns))

# Calculate metrics
mae = mean_absolute_error(y_test_flat, predictions_flat)
mse = mean_squared_error(y_test_flat, predictions_flat)
rmse = np.sqrt(mse)
r2 = r2_score(y_test_flat, predictions_flat)

# Function to display metrics with a UI
def display_metrics_ui(mae, mse, rmse, r2):
    output = widgets.Output()
    with output:
        clear_output(wait=True)
        print("Model Accuracy Metrics:")
        print(f"Mean Absolute Error (MAE): {mae}")
        print(f"Mean Squared Error (MSE): {mse}")
        print(f"Root Mean Squared Error (RMSE): {rmse}")
        print(f"R^2 Score: {r2}")
    display(output)

# Display metrics
display_metrics_ui(mae, mse, rmse, r2)

import pandas as pd
import numpy as np
import ipywidgets as widgets
from IPython.display import display, clear_output
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Example predicted weather data
predicted_weather = np.array([
    [50, 0.8, 0.7, 5],  # Day 1
    [60, 0.9, 0.8, 6],  # Day 2
    [70, 0.85, 0.75, 7] # Day 3
])

# Convert predicted weather data to DataFrame
predicted_weather_df = pd.DataFrame(predicted_weather, columns=['precip', 'precipprob', 'precipcover', 'windspeed'])

# Example actual test data (mocked for demonstration)
y_test = np.array([
    [55, 0.85, 0.7, 5, 0, 1],  # Example actual values
    [65, 0.9, 0.75, 6, 1, 0]   # Example actual values
])

# Convert y_test to DataFrame with the same features
y_test_df = pd.DataFrame(y_test, columns=['precip', 'precipprob', 'precipcover', 'windspeed', 'snow', 'other_feature'])

# Ensure predicted_weather_df has the same columns as y_test_df
for feature in y_test_df.columns:
    if feature not in predicted_weather_df.columns:
        predicted_weather_df[feature] = 0  # Add missing features

# Reorder columns to match y_test_df
predicted_weather_df = predicted_weather_df[y_test_df.columns]

# Ensure the number of samples matches
min_samples = min(len(y_test_df), len(predicted_weather_df))
y_test_df = y_test_df.iloc[:min_samples]
predicted_weather_df = predicted_weather_df.iloc[:min_samples]

# Flatten y_test and predictions
y_test_flat = y_test_df.values
predictions_flat = predicted_weather_df.values

# Calculate metrics
mae = mean_absolute_error(y_test_flat, predictions_flat)
mse = mean_squared_error(y_test_flat, predictions_flat)
rmse = np.sqrt(mse)
r2 = r2_score(y_test_flat, predictions_flat)

# Function to display metrics with a UI
def display_metrics_ui(mae, mse, rmse, r2):
    output = widgets.Output()
    with output:
        clear_output(wait=True)
        print("Model Accuracy Metrics:")
        print(f"Mean Absolute Error (MAE): {mae}")
        print(f"Mean Squared Error (MSE): {mse}")
        print(f"Root Mean Squared Error (RMSE): {rmse}")
        print(f"R^2 Score: {r2}")
    display(output)

# Display metrics
display_metrics_ui(mae, mse, rmse, r2)

import pandas as pd
import numpy as np
import geopandas as gpd
import matplotlib.pyplot as plt
from shapely.geometry import Point

# Example predicted weather data
predicted_weather = np.array([
    [50, 0.8, 0.7, 5],  # Day 1
    [60, 0.9, 0.8, 6],  # Day 2
    [70, 0.85, 0.75, 7] # Day 3
])

# Convert predicted weather data to DataFrame
predicted_weather_df = pd.DataFrame(predicted_weather, columns=['precip', 'precipprob', 'precipcover', 'windspeed'])

# Define flood risk threshold
flood_threshold = 50

# Determine if each day is flood-prone
predicted_weather_df['is_flood_prone'] = predicted_weather_df['precip'] >= flood_threshold

# Load Surat road network data
edges = gpd.read_file('/content/surat_road_network.shp')  # Adjust path as necessary

# Simulate flood-prone areas (replace this with real data if available)
# Generate random flood-prone points
flood_points = pd.DataFrame({
    'latitude': np.random.uniform(low=20.0, high=22.0, size=100),
    'longitude': np.random.uniform(low=72.0, high=74.0, size=100)
})

# Convert to GeoDataFrame
flood_gdf = gpd.GeoDataFrame(flood_points,
                              geometry=gpd.points_from_xy(flood_points.longitude, flood_points.latitude))

# Buffer the flood points to simulate flood areas
flood_gdf['geometry'] = flood_gdf.buffer(0.01)  # Approx. 1 km buffer

# Overlay flood areas on road network
flooded_edges = gpd.sjoin(edges, flood_gdf, how='inner', op='intersects')

# Plot results
fig, ax = plt.subplots(figsize=(12, 10))
base = edges.plot(ax=ax, color='blue', linewidth=0.5)
flooded_edges.plot(ax=base, color='red', linewidth=2, alpha=0.7, label='Flooded Roads')
flood_gdf.plot(ax=base, color='yellow', markersize=5, alpha=0.5, label='Flood-prone Areas')
plt.title('Flood Risk and Affected Road Network')
plt.xlabel('Longitude')
plt.ylabel('Latitude')
plt.legend()
plt.show()

# Example of changing the number of units in LSTM layers
model = Sequential()
model.add(LSTM(units=100, return_sequences=True, input_shape=(sequence_length, X_train.shape[2])))
model.add(Dropout(0.3))
model.add(LSTM(units=100, return_sequences=False))
model.add(Dropout(0.3))
model.add(Dense(units=30))
model.add(Dropout(0.3))
model.add(Dense(units=len(target_columns)))
model.compile(optimizer='adam', loss='mean_squared_error')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime, timedelta

# Example predicted weather data
predicted_weather = np.array([
    [50, 0.8, 0.7, 5],  # Day 1
    [60, 0.9, 0.8, 6],  # Day 2
    [70, 0.85, 0.75, 7] # Day 3
])

# Convert to DataFrame
predicted_weather_df = pd.DataFrame(predicted_weather, columns=['precip', 'precipprob', 'precipcover', 'windspeed'])

# Generate dates for the next 3 days
start_date = datetime.now()
dates = [start_date + timedelta(days=i) for i in range(3)]

# Convert dates to string format for x-axis labels
date_labels = [date.strftime('%Y-%m-%d') for date in dates]

# Plot predicted weather data
fig, ax = plt.subplots(figsize=(12, 6))
predicted_weather_df.plot(ax=ax, marker='o')

# Set x-axis labels to the generated dates
ax.set_xticks(range(len(date_labels)))
ax.set_xticklabels(date_labels, rotation=45)

plt.title('Predicted Weather Data for the Next 3 Days')
plt.xlabel('Date')
plt.ylabel('Value')
plt.legend(loc='best')
plt.grid(True)
plt.tight_layout()
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime, timedelta

# Example historical weather data for the previous 10 days
# You should replace this with your actual historical data
dates = [datetime.now() - timedelta(days=i) for i in range(10)]
historical_weather = np.random.rand(10, 4)  # Replace with actual weather data

# Create a DataFrame
historical_weather_df = pd.DataFrame(historical_weather, columns=['precip', 'precipprob', 'precipcover', 'windspeed'])
historical_weather_df['date'] = dates

# Plot historical weather data
fig, ax = plt.subplots(figsize=(12, 6))

# Plot each weather condition
for column in historical_weather_df.columns[:-1]:  # Exclude the 'date' column
    ax.plot(historical_weather_df['date'], historical_weather_df[column], marker='o', label=column)

# Format x-axis with dates
ax.set_xticks(historical_weather_df['date'])
ax.xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%Y-%m-%d'))
plt.xticks(rotation=45)

plt.title('Weather Conditions for the Previous 10 Days')
plt.xlabel('Date')
plt.ylabel('Value')
plt.legend(loc='best')
plt.grid(True)
plt.tight_layout()
plt.show()

import pandas as pd
import numpy as np
import geopandas as gpd
import matplotlib.pyplot as plt
from shapely.geometry import Point
import ipywidgets as widgets
from IPython.display import display, clear_output

# Example predicted weather data
predicted_weather = np.array([
    [50, 0.8, 0.7, 5],  # Day 1
    [60, 0.9, 0.8, 6],  # Day 2
    [70, 0.85, 0.75, 7] # Day 3
])

# Convert predicted weather data to DataFrame
predicted_weather_df = pd.DataFrame(predicted_weather, columns=['precip', 'precipprob', 'precipcover', 'windspeed'])

# Define flood risk threshold
flood_threshold = 50

# Determine if each day is flood-prone
predicted_weather_df['is_flood_prone'] = predicted_weather_df['precip'] >= flood_threshold

# Function to handle user input for flood points
def handle_flood_points(b):
    # Create an Output widget to capture and display content
    output = widgets.Output()
    with output:
        clear_output(wait=True)

        # Get user input for flood points
        flood_points = {
            'latitude': [float(lat) for lat in lat_textbox.value.split(',')],
            'longitude': [float(lon) for lon in lon_textbox.value.split(',')]
        }

        # Convert to DataFrame
        flood_df = pd.DataFrame(flood_points)

        # Convert to GeoDataFrame
        flood_gdf = gpd.GeoDataFrame(flood_df,
                                      geometry=gpd.points_from_xy(flood_df.longitude, flood_df.latitude))

        # Buffer the flood points to simulate flood areas
        flood_gdf['geometry'] = flood_gdf.buffer(0.01)  # Approx. 1 km buffer

        # Load Surat road network data
        edges = gpd.read_file('/content/surat_road_network.shp')  # Adjust path as necessary

        # Overlay flood areas on road network
        flooded_edges = gpd.sjoin(edges, flood_gdf, how='inner', op='intersects')

        # Plot results
        fig, ax = plt.subplots(figsize=(12, 10))
        base = edges.plot(ax=ax, color='blue', linewidth=0.5)
        flooded_edges.plot(ax=base, color='red', linewidth=2, alpha=0.7, label='Flooded Roads')
        flood_gdf.plot(ax=base, color='yellow', markersize=5, alpha=0.5, label='Flood-prone Areas')
        plt.title('Flood Risk and Affected Road Network')
        plt.xlabel('Longitude')
        plt.ylabel('Latitude')
        plt.legend()
        plt.show()

        print("Flood analysis completed with user-provided points.")

    # Display the output widget outside the 'with' block
    display(output)

# Create UI for user input
lat_textbox = widgets.Text(
    description='Latitude:',
    placeholder='Enter latitudes separated by commas'
)

lon_textbox = widgets.Text(
    description='Longitude:',
    placeholder='Enter longitudes separated by commas'
)

submit_button = widgets.Button(
    description='Submit'
)
submit_button.on_click(handle_flood_points)

# Display the widgets
display(lat_textbox, lon_textbox, submit_button)

# Display the predicted weather dataframe for reference
print("Predicted Weather Data:")
print(predicted_weather_df)

import pandas as pd
import numpy as np
import geopandas as gpd
import matplotlib.pyplot as plt
from shapely.geometry import Point
from ipywidgets import widgets
from IPython.display import display, clear_output

# Load road network data
edges = gpd.read_file('/content/surat_road_network.shp')  # Adjust path as necessary

# Ensure road network data is in WGS84 coordinate system (EPSG:4326)
if edges.crs != "EPSG:4326":
    edges = edges.to_crs(epsg=4326)

# Example predicted weather data
predicted_weather = np.array([
    [50, 0.8, 0.7, 5],  # Day 1
    [60, 0.9, 0.8, 6],  # Day 2
    [70, 0.85, 0.75, 7] # Day 3
])

# Convert to DataFrame
predicted_weather_df = pd.DataFrame(predicted_weather, columns=['precip', 'precipprob', 'precipcover', 'windspeed'])

# Determine flood-prone areas based on precipitation
flood_threshold = 50
predicted_weather_df['is_flood_prone'] = predicted_weather_df['precip'] >= flood_threshold

# Function to simulate flood areas around input coordinates
def simulate_flood_areas(lat, lon, buffer_size=0.01):
    flood_points = pd.DataFrame({
        'latitude': [lat],
        'longitude': [lon]
    })
    flood_gdf = gpd.GeoDataFrame(flood_points,
                                  geometry=gpd.points_from_xy(flood_points.longitude, flood_points.latitude),
                                  crs="EPSG:4326")
    flood_gdf['geometry'] = flood_gdf.buffer(buffer_size)  # Buffer area to simulate flood zones
    return flood_gdf

# Function to visualize the flood path
def visualize_flood_path(lat, lon):
    # Simulate flood areas
    flood_gdf = simulate_flood_areas(lat, lon)

    # Overlay flood areas on road network
    flooded_edges = gpd.sjoin(edges, flood_gdf, how='inner', op='intersects')

    # Plot results
    fig, ax = plt.subplots(figsize=(12, 10))
    base = edges.plot(ax=ax, color='blue', linewidth=0.5, label='Road Network')
    flooded_edges.plot(ax=base, color='red', linewidth=2, alpha=0.7, label='Flooded Roads')
    flood_gdf.plot(ax=base, color='yellow', markersize=50, alpha=0.5, label='Flood-prone Areas')

    # Set plot limits
    ax.set_xlim([edges.bounds.minx.min(), edges.bounds.maxx.max()])
    ax.set_ylim([edges.bounds.miny.min(), edges.bounds.maxy.max()])

    # Set aspect ratio to be equal
    ax.set_aspect('equal')

    plt.title('Flood Risk and Affected Road Network')
    plt.xlabel('Longitude')
    plt.ylabel('Latitude')
    plt.legend()
    plt.grid(True)
    plt.show()

# UI to get user input and visualize flood path
def get_user_input_and_visualize():
    # Create UI elements
    lat_input = widgets.FloatText(description='Latitude:', value=20.0)
    lon_input = widgets.FloatText(description='Longitude:', value=72.0)
    button = widgets.Button(description="Show Map")

    # Define button click event
    def on_button_click(b):
        lat = lat_input.value
        lon = lon_input.value
        # Validate latitude and longitude
        if not (-90 <= lat <= 90 and -180 <= lon <= 180):
            print("Invalid coordinates. Please enter values within the valid range.")
            return
        visualize_flood_path(lat, lon)

    button.on_click(on_button_click)

    # Display UI elements
    display(lat_input, lon_input, button)

# Call the function to display UI
get_user_input_and_visualize()